{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather forecast - dataset generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, climate data provided by the DWD will be transferred to a local sqlite database. This climate information is provided free of charge (due to legal obligations) by the DWD at the following address: https://opendata.dwd.de/climate_environment/CDC/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from ftplib import FTP\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General settings/parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the data that will be received\n",
    "CreateDatabase = True\n",
    "IncludeHistoricalData = True\n",
    "IncludeRecentData = True\n",
    "\n",
    "# select different stations (feel free to add/remove stations)\n",
    "stations = ['02483', '02437', '02315', '02261', '02171', '01975']\n",
    "\n",
    "# select start year\n",
    "# Note: Make sure the startYear is >= 1970 because the timestamp will be converted into unix timestamp\n",
    "recordStartYear = 2010\n",
    "\n",
    "# general settings\n",
    "path = 'opendata.dwd.de'\n",
    "databaseScriptName = 'database.sql'\n",
    "databaseName = 'weather.db'\n",
    "workDirectory = os.path.join(pathlib.Path().resolve(), 'raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an empty sql database using a sql script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete old SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (CreateDatabase):\n",
    "    dbFilePath = os.path.join(pathlib.Path().resolve(), databaseName)\n",
    "    if (os.path.exists(dbFilePath)):\n",
    "        os.remove(dbFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SQL Script to create a new SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (CreateDatabase):\n",
    "    with open(databaseScriptName, 'r') as sqlFile:\n",
    "        sqlScript = sqlFile.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to insert master data into the SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to insert the stations master data\n",
    "def StationsMasterData(filePath, dbCursor):\n",
    "    print('Insert stations master data into local database')\n",
    "    if (os.path.exists(filePath)):\n",
    "        with open(filePath, 'r') as stationFile:\n",
    "            cmd = 'INSERT OR IGNORE INTO Stations(StationIdent, Description, State, SeaLevel, Latitude, Longitude) VALUES (?, ?, ?, ?, ?, ?)'\n",
    "            lines = stationFile.readlines()\n",
    "            for line in lines:\n",
    "                line = line.strip() # remove leading and trailing whitespace\n",
    "                line = re.sub(' +', ' ', line) # remove all inline whitespace except one\n",
    "                lineSplit = line.split(';')\n",
    "                if (len(lineSplit) == 8):\n",
    "                    stationIdent = int(lineSplit[0].strip())\n",
    "                    seaLevel = float(lineSplit[3].strip())\n",
    "                    latitude = float(lineSplit[4].strip())\n",
    "                    longitude = float(lineSplit[5].strip())\n",
    "                    description = lineSplit[6].strip()\n",
    "                    state = lineSplit[7].strip()\n",
    "\n",
    "                    dbCursor.execute(cmd, (stationIdent, description, state, seaLevel, latitude, longitude))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WeatherPhenomenaMasterData(filePath, dbCursor):\n",
    "    print('Insert weather phenomena master data into local database')\n",
    "    if (os.path.exists(filePath)):\n",
    "        with open(filePath, 'r') as stationFile:\n",
    "            cmd = 'INSERT OR IGNORE INTO WeatherPhenomena(WeatherPhenomenonIdent, Description) VALUES (?, ?)'\n",
    "            lines = stationFile.readlines()\n",
    "            for line in lines:\n",
    "                line = line.strip() # remove leading and trailing whitespace\n",
    "                line = re.sub(' +', ' ', line) # remove all inline whitespace except one\n",
    "                lineSplit = line.split(';')\n",
    "                if (len(lineSplit) == 4):\n",
    "                    phenomenonIdent = int(lineSplit[0].strip())\n",
    "                    description = lineSplit[1].strip()\n",
    "\n",
    "                    dbCursor.execute(cmd, (phenomenonIdent, description))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QualityLevelMasterData(filePath, dbCursor):\n",
    "    print('Insert quality level master data into local database')\n",
    "    if (os.path.exists(filePath)):\n",
    "        with open(filePath, 'r') as qualityLevelFile:\n",
    "            cmd = 'INSERT OR IGNORE INTO QualityLevels(QLIdent, Description) VALUES (?, ?)'\n",
    "            lines = qualityLevelFile.readlines()\n",
    "            for line in lines:\n",
    "                line = line.strip() # remove leading and trailing whitespace\n",
    "                line = re.sub(' +', ' ', line) # remove all inline whitespace except one\n",
    "                lineSplit = line.split(';')\n",
    "                if (len(lineSplit) == 2):\n",
    "                    qlIdent = int(lineSplit[0].strip())\n",
    "                    description = lineSplit[1].strip()\n",
    "\n",
    "                    dbCursor.execute(cmd, (qlIdent, description)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QualityByteMasterData(filePath, dbCursor):\n",
    "    print('Insert quality byte master data into local database')\n",
    "    if (os.path.exists(filePath)):\n",
    "        with open(filePath, 'r') as qualityLevelFile:\n",
    "            cmd = 'INSERT OR IGNORE INTO QualityBytes(QBIdent, Description) VALUES (?, ?)'\n",
    "            lines = qualityLevelFile.readlines()\n",
    "            for line in lines:\n",
    "                line = line.strip() # remove leading and trailing whitespace\n",
    "                line = re.sub(' +', ' ', line) # remove all inline whitespace except one\n",
    "                lineSplit = line.split(';')\n",
    "                if (len(lineSplit) == 2):\n",
    "                    qbIdent = int(lineSplit[0].strip())\n",
    "                    description = lineSplit[1].strip()\n",
    "\n",
    "                    dbCursor.execute(cmd, (qbIdent, description)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert stations master data into local database\n",
      "Insert weather phenomena master data into local database\n",
      "Insert quality level master data into local database\n",
      "Insert quality byte master data into local database\n"
     ]
    }
   ],
   "source": [
    "# execute the sql script\n",
    "if (CreateDatabase):\n",
    "    db = sqlite3.connect(databaseName)\n",
    "    cursor = db.cursor()\n",
    "    cursor.executescript(sqlScript)\n",
    "    db.commit()\n",
    "\n",
    "    # insert master data\n",
    "    workDirectory = os.path.join(pathlib.Path().resolve(), 'raw')\n",
    "    StationsMasterData(os.path.join(workDirectory, 'stations.txt'), cursor)\n",
    "    WeatherPhenomenaMasterData(os.path.join(workDirectory, 'weather_phenomena.txt'), cursor)\n",
    "    QualityLevelMasterData(os.path.join(workDirectory, 'quality_levels.txt'), cursor)\n",
    "    QualityByteMasterData(os.path.join(workDirectory, 'quality_bytes.txt'), cursor)\n",
    "    db.commit()\n",
    "\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDateTime(timestamp):\n",
    "    # dwd timestring consists of 1950040105 = 1950.04.01 05:00:00\n",
    "    year = int(timestamp[0:4])\n",
    "    month = int(timestamp[4:6])\n",
    "    day = int(timestamp[6:8])\n",
    "    hour = int(timestamp[8:10])\n",
    "    return datetime(year, month, day, hour, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterFileList(filters, files):\n",
    "    filteredFiles = []\n",
    "    for file in files:\n",
    "        for filter in filters:\n",
    "            if filter in file:\n",
    "                if file.endswith('.zip'):\n",
    "                    filteredFiles.append(file)\n",
    "    return filteredFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy definition\n",
    "def ProcessFileLine(line, offsetYear):\n",
    "    # process and insert line into database\n",
    "    try:\n",
    "        # dummy parameter\n",
    "        if (len(line) == 6):\n",
    "            stationID = int(line[0]) # station identification number\n",
    "            timestamp = CreateDateTime(line[1]) # measurement time\n",
    "            eor = line[2] # end of record\n",
    "\n",
    "            if (int(timestamp.year) >= offsetYear):\n",
    "                # create unix timestamp\n",
    "                unixTimestamp = int(timestamp.timestamp())\n",
    "\n",
    "                # save line in database\n",
    "                return (stationID, timestamp, unixTimestamp)\n",
    "            else:\n",
    "                # return nothing in case of an error\n",
    "                return None\n",
    "    except:\n",
    "        print('Error processing line:', line)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessFTPData(files, fileFilter, offsetYear, sqlCommand, workDirectory, database, lineProcessor, utf8=True):\n",
    "    # count files to report current process\n",
    "    fileIndex = 1\n",
    "    fileCount = len(files)\n",
    "    \n",
    "    # db cursor\n",
    "    cursor = database.cursor()\n",
    "\n",
    "    # process each filtered file (only selected stations)\n",
    "    for file in files:\n",
    "        print('Process file', fileIndex, 'of', fileCount, '-', file)\n",
    "\n",
    "        try:\n",
    "            # retrieve file\n",
    "            zipFilePath = os.path.join(workDirectory, file)\n",
    "            ftp.retrbinary(\"RETR \" + file, open(zipFilePath, 'wb').write)\n",
    "\n",
    "            # unzip file\n",
    "            with ZipFile(zipFilePath) as zf:\n",
    "                hasHeader = True\n",
    "\n",
    "                # find the data file with certain name\n",
    "                matchingFiles = fnmatch.filter(zf.namelist(), fileFilter)\n",
    "                # only proceed when file name is found\n",
    "                if len(matchingFiles) >= 1:    \n",
    "                    # only the first file will be opened and processed                \n",
    "                    fileName = matchingFiles[0]\n",
    "                    with zf.open(fileName, 'r') as datafile:\n",
    "                        # read lines\n",
    "                        lines = datafile.readlines()\n",
    "\n",
    "                        # process lines in file\n",
    "                        lineCount = 0\n",
    "                        for line in lines:\n",
    "                            if (lineCount > 0 or not hasHeader):\n",
    "                                if utf8:                      \n",
    "                                    line = line.decode('utf-8').rstrip()\n",
    "                                else:\n",
    "                                    line = line.decode('ISO-8859-1').rstrip()\n",
    "                                line = [x.strip() for x in line.split(';')]\n",
    "\n",
    "                                result = lineProcessor(line, offsetYear)\n",
    "                                if (result is not None):\n",
    "                                    cursor.execute(sqlCommand, result)\n",
    "                                    \n",
    "                            # count lines processed      \n",
    "                            lineCount = lineCount + 1   \n",
    "\n",
    "            # clean up and remove each retrieved file after usage\n",
    "            os.remove(zipFilePath)\n",
    "\n",
    "            # count processed files\n",
    "            fileIndex = fileIndex + 1\n",
    "        except Exception as e:\n",
    "            print('An error occured while processing file', file)\n",
    "            print('Error:', e)   \n",
    "\n",
    "    # close db cursor\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to DWD FTP Server and local SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"220 Welcome to DWD's opendata FTP service.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to local database\n",
    "db = sqlite3.connect(databaseName)\n",
    "\n",
    "# connect and login to ftp server\n",
    "ftp = FTP(path)  # connect to host, default port\n",
    "ftp.login()\n",
    "ftp.getwelcome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station observations of 2m air temperature and humidity for Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General settings/parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathHistorical = '/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/historical'\n",
    "pathRecent = '/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent'\n",
    "sqlCommand = 'INSERT OR IGNORE INTO AirTemperature(StationIdent, QLIdent, DateMeasuredUTC, DateMeasuredUnixUTC, TT_TU, RF_TU) VALUES (?, ?, ?, ?, ?, ?)'\n",
    "dataFileFilter = 'produkt_tu_stunde*.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessFileLine(line, offsetYear):\n",
    "    # process and insert line into database\n",
    "    try:\n",
    "        # access the parameters of each line\n",
    "        if (len(line) == 6):\n",
    "            stationID = int(line[0]) # station identification number\n",
    "            timestamp = CreateDateTime(line[1]) # measurement time\n",
    "            qn_9 = int(line[2]) # quality level of next columns\n",
    "            tt_tu = float(line[3]) # 2m air temperature\n",
    "            rf_tu = float(line[4]) # 2m relative humidity \n",
    "            eor = line[5] # end of record\n",
    "\n",
    "            if (int(timestamp.year) >= offsetYear):\n",
    "                # create unix timestamp\n",
    "                unixTimestamp = int(timestamp.timestamp())\n",
    "\n",
    "                # save line in database\n",
    "                return (stationID, qn_9, timestamp, unixTimestamp, tt_tu, rf_tu)\n",
    "            else:\n",
    "                # return nothing in case of an error\n",
    "                return None\n",
    "    except:\n",
    "        print('Error processing line:', line)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 6 - stundenwerte_TU_01975_19490101_20211231_hist.zip\n",
      "Process file 2 of 6 - stundenwerte_TU_02171_19510101_20211231_hist.zip\n",
      "Process file 3 of 6 - stundenwerte_TU_02261_19480101_20211231_hist.zip\n",
      "Process file 4 of 6 - stundenwerte_TU_02315_20021101_20211231_hist.zip\n",
      "Process file 5 of 6 - stundenwerte_TU_02437_20020101_20211231_hist.zip\n",
      "Process file 6 of 6 - stundenwerte_TU_02483_19510101_20211231_hist.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeHistoricalData):\n",
    "    # move to ftp directory and retrieve files\n",
    "    ftp.cwd(pathHistorical)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 6 - stundenwerte_TU_01975_akt.zip\n",
      "Process file 2 of 6 - stundenwerte_TU_02171_akt.zip\n",
      "Process file 3 of 6 - stundenwerte_TU_02261_akt.zip\n",
      "Process file 4 of 6 - stundenwerte_TU_02315_akt.zip\n",
      "Process file 5 of 6 - stundenwerte_TU_02437_akt.zip\n",
      "Process file 6 of 6 - stundenwerte_TU_02483_akt.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeRecentData):\n",
    "    # move to ftp directory\n",
    "    ftp.cwd(pathRecent)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloudiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station observations of cloudiness for German"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General settings/parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathHistorical = '/climate_environment/CDC/observations_germany/climate/hourly/cloudiness/historical'\n",
    "pathRecent = '/climate_environment/CDC/observations_germany/climate/hourly/cloudiness/recent'\n",
    "dataFileFilter = 'produkt_n_stunde*.txt'\n",
    "sqlCommand = 'INSERT OR IGNORE INTO Cloudiness(StationIdent, QLIdent, DateMeasuredUTC, DateMeasuredUnixUTC, V_N_I, V_N) VALUES (?, ?, ?, ?, ?, ?)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessFileLine(line, offsetYear):\n",
    "    # process and insert line into database\n",
    "    try:\n",
    "        # access the parameters of each line\n",
    "        if (len(line) == 6):\n",
    "            stationID = int(line[0]) # station identification number\n",
    "            timestamp = CreateDateTime(line[1]) # measurement time\n",
    "            qn_8 = int(line[2]) # quality level of next columns\n",
    "            v_n_i = line[3].strip()\n",
    "            v_n = int(line[4])\n",
    "            eor = line[5] # end of record\n",
    "\n",
    "            if (int(timestamp.year) >= offsetYear):\n",
    "                # create unix timestamp\n",
    "                unixTimestamp = int(timestamp.timestamp())\n",
    "\n",
    "                # save line in database\n",
    "                return (stationID, qn_8, timestamp, unixTimestamp, v_n_i, v_n)\n",
    "            else:\n",
    "                # return nothing in case of an error\n",
    "                return None\n",
    "    except:\n",
    "        print('Error processing line:', line)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 6 - stundenwerte_N_01975_19490101_20211231_hist.zip\n",
      "Process file 2 of 6 - stundenwerte_N_02171_19520901_20211231_hist.zip\n",
      "Process file 3 of 6 - stundenwerte_N_02261_19490101_20211231_hist.zip\n",
      "Process file 4 of 6 - stundenwerte_N_02315_19911101_20211231_hist.zip\n",
      "Process file 5 of 6 - stundenwerte_N_02437_19790101_20211231_hist.zip\n",
      "Process file 6 of 6 - stundenwerte_N_02483_19541207_20211231_hist.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeHistoricalData):\n",
    "    # move to ftp directory and retrieve files\n",
    "    ftp.cwd(pathHistorical)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 6 - stundenwerte_N_01975_akt.zip\n",
      "Process file 2 of 6 - stundenwerte_N_02171_akt.zip\n",
      "Process file 3 of 6 - stundenwerte_N_02261_akt.zip\n",
      "Process file 4 of 6 - stundenwerte_N_02315_akt.zip\n",
      "Process file 5 of 6 - stundenwerte_N_02437_akt.zip\n",
      "Process file 6 of 6 - stundenwerte_N_02483_akt.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeRecentData):\n",
    "    # move to ftp directory\n",
    "    ftp.cwd(pathRecent)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dew point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station observations of air and dew point temperature 2 m above ground in Â°C for Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General settings/parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathHistorical = '/climate_environment/CDC/observations_germany/climate/hourly/dew_point/historical'\n",
    "pathRecent = '/climate_environment/CDC/observations_germany/climate/hourly/dew_point/recent'\n",
    "dataFileFilter = 'produkt_td_stunde*.txt'\n",
    "sqlCommand = 'INSERT OR IGNORE INTO DewPoint(StationIdent, QLIdent, DateMeasuredUTC, DateMeasuredUnixUTC, TT, TD) VALUES (?, ?, ?, ?, ?, ?)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessFileLine(line, offsetYear):\n",
    "    # process and insert line into database\n",
    "    try:\n",
    "        # access the parameters of each line\n",
    "        if (len(line) == 6):\n",
    "            stationID = int(line[0]) # station identification number\n",
    "            timestamp = CreateDateTime(line[1]) # measurement time\n",
    "            qn_8 = int(line[2]) # quality level of next columns\n",
    "            tt = float(line[3])\n",
    "            td = float(line[4])\n",
    "            eor = line[5] # end of record\n",
    "\n",
    "            if (int(timestamp.year) >= offsetYear):\n",
    "                # create unix timestamp\n",
    "                unixTimestamp = int(timestamp.timestamp())\n",
    "\n",
    "                # save line in database\n",
    "                return (stationID, qn_8, timestamp, unixTimestamp, tt, td)\n",
    "            else:\n",
    "                # return nothing in case of an error\n",
    "                return None\n",
    "    except:\n",
    "        print('Error processing line:', line)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 6 - stundenwerte_TD_01975_19490101_20211231_hist.zip\n",
      "Process file 2 of 6 - stundenwerte_TD_02171_19520901_20211231_hist.zip\n",
      "Process file 3 of 6 - stundenwerte_TD_02261_19490101_20211231_hist.zip\n",
      "Process file 4 of 6 - stundenwerte_TD_02315_19911101_20211231_hist.zip\n",
      "Process file 5 of 6 - stundenwerte_TD_02437_19790101_20211231_hist.zip\n",
      "Process file 6 of 6 - stundenwerte_TD_02483_19541207_20211231_hist.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeHistoricalData):\n",
    "    # move to ftp directory and retrieve files\n",
    "    ftp.cwd(pathHistorical)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 6 - stundenwerte_TD_01975_akt.zip\n",
      "Process file 2 of 6 - stundenwerte_TD_02171_akt.zip\n",
      "Process file 3 of 6 - stundenwerte_TD_02261_akt.zip\n",
      "Process file 4 of 6 - stundenwerte_TD_02315_akt.zip\n",
      "Process file 5 of 6 - stundenwerte_TD_02437_akt.zip\n",
      "Process file 6 of 6 - stundenwerte_TD_02483_akt.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeRecentData):\n",
    "    # move to ftp directory\n",
    "    ftp.cwd(pathRecent)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moisture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General settings/parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathHistorical = '/climate_environment/CDC/observations_germany/climate/hourly/moisture/historical'\n",
    "pathRecent = '/climate_environment/CDC/observations_germany/climate/hourly/moisture/recent'\n",
    "dataFileFilter = 'produkt_tf_stunde*.txt'\n",
    "sqlCommand = 'INSERT OR IGNORE INTO Moisture(StationIdent, QLIdent, DateMeasuredUTC, DateMeasuredUnixUTC, ABSF_STD, VP_STD, TF_STD, P_STD, TT_STD, RF_STD, TD_STD) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessFileLine(line, offsetYear):\n",
    "    # process and insert line into database\n",
    "    try:\n",
    "        # access the parameters of each line\n",
    "        if (len(line) == 11):\n",
    "            stationID = int(line[0]) # station identification number\n",
    "            timestamp = CreateDateTime(line[1]) # measurement time\n",
    "            qn_8 = int(line[2]) # quality level of next columns\n",
    "            absf_td = float(line[3])\n",
    "            vp_std = float(line[4])\n",
    "            tf_std = float(line[5])\n",
    "            p_std = float(line[6])\n",
    "            tt_std = float(line[7])\n",
    "            rf_std = float(line[8])\n",
    "            td_std = float(line[9])\n",
    "            eor = line[10] # end of record\n",
    "\n",
    "            if (int(timestamp.year) >= offsetYear):\n",
    "                # create unix timestamp\n",
    "                unixTimestamp = int(timestamp.timestamp())\n",
    "\n",
    "                # save line in database\n",
    "                return (stationID, qn_8, timestamp, unixTimestamp, absf_td, vp_std, tf_std, p_std, tt_std, rf_std, td_std)\n",
    "            else:\n",
    "                # return nothing in case of an error\n",
    "                return None\n",
    "    except:\n",
    "        print('Error processing line:', line)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 6 - stundenwerte_TF_01975_19490101_20211231_hist.zip\n",
      "Process file 2 of 6 - stundenwerte_TF_02171_19520901_20211231_hist.zip\n",
      "Process file 3 of 6 - stundenwerte_TF_02261_19490101_20211231_hist.zip\n",
      "Process file 4 of 6 - stundenwerte_TF_02315_19911101_20211231_hist.zip\n",
      "Process file 5 of 6 - stundenwerte_TF_02437_19790101_20211231_hist.zip\n",
      "Process file 6 of 6 - stundenwerte_TF_02483_19541207_20211231_hist.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeHistoricalData):\n",
    "    # move to ftp directory and retrieve files\n",
    "    ftp.cwd(pathHistorical)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 6 - stundenwerte_TF_01975_akt.zip\n",
      "Process file 2 of 6 - stundenwerte_TF_02171_akt.zip\n",
      "Process file 3 of 6 - stundenwerte_TF_02261_akt.zip\n",
      "Process file 4 of 6 - stundenwerte_TF_02315_akt.zip\n",
      "Process file 5 of 6 - stundenwerte_TF_02437_akt.zip\n",
      "Process file 6 of 6 - stundenwerte_TF_02483_akt.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeRecentData):\n",
    "    # move to ftp directory\n",
    "    ftp.cwd(pathRecent)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station observations of precipitation for Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General settings/parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathHistorical = '/climate_environment/CDC/observations_germany/climate/hourly/precipitation/historical'\n",
    "pathRecent = '/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent'\n",
    "dataFileFilter = 'produkt_rr_stunde*.txt'\n",
    "sqlCommand = 'INSERT OR IGNORE INTO Precipitation(StationIdent, QLIdent, DateMeasuredUTC, DateMeasuredUnixUTC, R1, RS_IND) VALUES (?, ?, ?, ?, ?, ?)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessFileLine(line, offsetYear):\n",
    "    # process and insert line into database\n",
    "    try:\n",
    "        # access the parameters of each line\n",
    "        if (len(line) == 7):\n",
    "            stationID = int(line[0]) # station identification number\n",
    "            timestamp = CreateDateTime(line[1]) # measurement time\n",
    "            qn_8 = int(line[2]) # quality level of next columns\n",
    "            r1 = float(line[3])\n",
    "            rs_ind = float(line[4])\n",
    "            # wrtr will be irgnored here\n",
    "            eor = line[6] # end of record\n",
    "\n",
    "            if (int(timestamp.year) >= offsetYear):\n",
    "                # create unix timestamp\n",
    "                unixTimestamp = int(timestamp.timestamp())\n",
    "\n",
    "                # save line in database\n",
    "                return (stationID, qn_8, timestamp, unixTimestamp, r1, rs_ind)\n",
    "            else:\n",
    "                # return nothing in case of an error\n",
    "                return None\n",
    "    except:\n",
    "        print('Error processing line:', line)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 4 - stundenwerte_RR_01975_19950905_20211231_hist.zip\n",
      "Process file 2 of 4 - stundenwerte_RR_02171_19950901_20211231_hist.zip\n",
      "Process file 3 of 4 - stundenwerte_RR_02261_19991202_20211231_hist.zip\n",
      "Process file 4 of 4 - stundenwerte_RR_02483_19951012_20211231_hist.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeHistoricalData):\n",
    "    # move to ftp directory and retrieve files\n",
    "    ftp.cwd(pathHistorical)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 4 - stundenwerte_RR_01975_akt.zip\n",
      "Process file 2 of 4 - stundenwerte_RR_02171_akt.zip\n",
      "Process file 3 of 4 - stundenwerte_RR_02261_akt.zip\n",
      "Process file 4 of 4 - stundenwerte_RR_02483_akt.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeRecentData):\n",
    "    # move to ftp directory\n",
    "    ftp.cwd(pathRecent)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station observations of pressure for Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General settings/parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathHistorical = '/climate_environment/CDC/observations_germany/climate/hourly/pressure/historical'\n",
    "pathRecent = '/climate_environment/CDC/observations_germany/climate/hourly/pressure/recent'\n",
    "dataFileFilter = 'produkt_p0_stunde*.txt'\n",
    "sqlCommand = 'INSERT OR IGNORE INTO Pressure(StationIdent, QLIdent, DateMeasuredUTC, DateMeasuredUnixUTC, P, P0) VALUES (?, ?, ?, ?, ?, ?)'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessFileLine(line, offsetYear):\n",
    "    # process and insert line into database\n",
    "    try:\n",
    "        # access the parameters of each line\n",
    "        if (len(line) == 6):\n",
    "            stationID = int(line[0]) # station identification number\n",
    "            timestamp = CreateDateTime(line[1]) # measurement time\n",
    "            qn_8 = int(line[2]) # quality level of next columns\n",
    "            p = float(line[3])\n",
    "            p0 = float(line[4])\n",
    "            eor = line[5] # end of record\n",
    "\n",
    "            if (int(timestamp.year) >= offsetYear):\n",
    "                # create unix timestamp\n",
    "                unixTimestamp = int(timestamp.timestamp())\n",
    "\n",
    "                # save line in database\n",
    "                return (stationID, qn_8, timestamp, unixTimestamp, p, p0)\n",
    "            else:\n",
    "                # return nothing in case of an error\n",
    "                return None\n",
    "    except:\n",
    "        print('Error processing line:', line)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 6 - stundenwerte_P0_01975_19490101_20211231_hist.zip\n",
      "Process file 2 of 6 - stundenwerte_P0_02171_19520901_20211231_hist.zip\n",
      "Process file 3 of 6 - stundenwerte_P0_02261_19490101_20211231_hist.zip\n",
      "Process file 4 of 6 - stundenwerte_P0_02315_19911101_20211231_hist.zip\n",
      "Process file 5 of 6 - stundenwerte_P0_02437_19790101_20211231_hist.zip\n",
      "Process file 6 of 6 - stundenwerte_P0_02483_19541207_20211231_hist.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeHistoricalData):\n",
    "    # move to ftp directory and retrieve files\n",
    "    ftp.cwd(pathHistorical)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 6 - stundenwerte_P0_01975_akt.zip\n",
      "Process file 2 of 6 - stundenwerte_P0_02171_akt.zip\n",
      "Process file 3 of 6 - stundenwerte_P0_02261_akt.zip\n",
      "Process file 4 of 6 - stundenwerte_P0_02315_akt.zip\n",
      "Process file 5 of 6 - stundenwerte_P0_02437_akt.zip\n",
      "Process file 6 of 6 - stundenwerte_P0_02483_akt.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeRecentData):\n",
    "    # move to ftp directory\n",
    "    ftp.cwd(pathRecent)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soil Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station observations of soil temperature station data for Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General settings/parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathHistorical = '/climate_environment/CDC/observations_germany/climate/hourly/soil_temperature/historical'\n",
    "pathRecent = '/climate_environment/CDC/observations_germany/climate/hourly/soil_temperature/recent'\n",
    "dataFileFilter = 'produkt_eb_stunde*.txt'\n",
    "sqlCommand = 'INSERT OR IGNORE INTO SoilTemperature(StationIdent, QLIdent, DateMeasuredUTC, DateMeasuredUnixUTC, V_TE002, V_TE005, V_TE010, V_TE020, V_TE050, V_TE100) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessFileLine(line, offsetYear):\n",
    "    # process and insert line into database\n",
    "    try:\n",
    "        # access the parameters of each line\n",
    "        if (len(line) == 10):\n",
    "            stationID = int(line[0]) # station identification number\n",
    "            timestamp = CreateDateTime(line[1]) # measurement time\n",
    "            qn_2 = int(line[2]) # quality level of next columns\n",
    "            v_te002 = float(line[3])\n",
    "            v_te005 = float(line[4])\n",
    "            v_te010 = float(line[5])\n",
    "            v_te020 = float(line[6])\n",
    "            v_te050 = float(line[7])\n",
    "            v_te100 = float(line[8])\n",
    "            eor = line[9] # end of record\n",
    "\n",
    "            if (int(timestamp.year) >= offsetYear):\n",
    "                # create unix timestamp\n",
    "                unixTimestamp = int(timestamp.timestamp())\n",
    "\n",
    "                # save line in database\n",
    "                return (stationID, qn_2, timestamp, unixTimestamp, v_te002, v_te005, v_te010, v_te020, v_te050, v_te100)\n",
    "            else:\n",
    "                # return nothing in case of an error\n",
    "                return None\n",
    "    except:\n",
    "        print('Error processing line:', line)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 6 - stundenwerte_EB_01975_19610101_20211231_hist.zip\n",
      "Process file 2 of 6 - stundenwerte_EB_02171_19810101_20211231_hist.zip\n",
      "Process file 3 of 6 - stundenwerte_EB_02261_19510101_20211231_hist.zip\n",
      "Process file 4 of 6 - stundenwerte_EB_02315_20010403_20211231_hist.zip\n",
      "Process file 5 of 6 - stundenwerte_EB_02437_19920101_20211231_hist.zip\n",
      "Process file 6 of 6 - stundenwerte_EB_02483_19810101_20211231_hist.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeHistoricalData):\n",
    "    # move to ftp directory and retrieve files\n",
    "    ftp.cwd(pathHistorical)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 6 - stundenwerte_EB_01975_akt.zip\n",
      "Process file 2 of 6 - stundenwerte_EB_02171_akt.zip\n",
      "Process file 3 of 6 - stundenwerte_EB_02261_akt.zip\n",
      "Process file 4 of 6 - stundenwerte_EB_02315_akt.zip\n",
      "Process file 5 of 6 - stundenwerte_EB_02437_akt.zip\n",
      "Process file 6 of 6 - stundenwerte_EB_02483_akt.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeRecentData):\n",
    "    # move to ftp directory\n",
    "    ftp.cwd(pathRecent)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station observations of sunshine duration for Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General settings/parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathHistorical = '/climate_environment/CDC/observations_germany/climate/hourly/sun/historical'\n",
    "pathRecent = '/climate_environment/CDC/observations_germany/climate/hourly/sun/recent'\n",
    "dataFileFilter = 'produkt_sd_stunde*.txt'\n",
    "sqlCommand = 'INSERT OR IGNORE INTO Sun(StationIdent, QLIdent, DateMeasuredUTC, DateMeasuredUnixUTC, SD_SO) VALUES (?, ?, ?, ?, ?)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessFileLine(line, offsetYear):\n",
    "    # process and insert line into database\n",
    "    try:\n",
    "        # access the parameters of each line\n",
    "        if (len(line) == 5):\n",
    "            stationID = int(line[0]) # station identification number\n",
    "            timestamp = CreateDateTime(line[1]) # measurement time\n",
    "            qn_7 = int(line[2]) # quality level of next columns\n",
    "            sd_so = float(line[3])\n",
    "            eor = line[4] # end of record\n",
    "\n",
    "            if (int(timestamp.year) >= offsetYear):\n",
    "                # create unix timestamp\n",
    "                unixTimestamp = int(timestamp.timestamp())\n",
    "\n",
    "                # save line in database\n",
    "                return (stationID, qn_7, timestamp, unixTimestamp, sd_so)\n",
    "            else:\n",
    "                # return nothing in case of an error\n",
    "                return None\n",
    "    except:\n",
    "        print('Error processing line:', line)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 5 - stundenwerte_SD_01975_19490101_20211231_hist.zip\n",
      "Process file 2 of 5 - stundenwerte_SD_02171_19510101_20211231_hist.zip\n",
      "Process file 3 of 5 - stundenwerte_SD_02261_19510101_20211231_hist.zip\n",
      "Process file 4 of 5 - stundenwerte_SD_02437_19880301_20061231_hist.zip\n",
      "Process file 5 of 5 - stundenwerte_SD_02483_19610101_20211231_hist.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeHistoricalData):\n",
    "    # move to ftp directory and retrieve files\n",
    "    ftp.cwd(pathHistorical)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 4 - stundenwerte_SD_01975_akt.zip\n",
      "Process file 2 of 4 - stundenwerte_SD_02171_akt.zip\n",
      "Process file 3 of 4 - stundenwerte_SD_02261_akt.zip\n",
      "Process file 4 of 4 - stundenwerte_SD_02483_akt.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeRecentData):\n",
    "    # move to ftp directory\n",
    "    ftp.cwd(pathRecent)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Phenomena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General settings/parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathHistorical = '/climate_environment/CDC/observations_germany/climate/hourly/weather_phenomena/historical'\n",
    "pathRecent = '/climate_environment/CDC/observations_germany/climate/hourly/weather_phenomena/recent'\n",
    "dataFileFilter = 'produkt_ww_stunde*.txt'\n",
    "sqlCommand = 'INSERT OR IGNORE INTO WeatherPhenomenaData(StationIdent, QLIdent, DateMeasuredUTC, DateMeasuredUnixUTC, WW) VALUES (?, ?, ?, ?, ?)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessFileLine(line, offsetYear):\n",
    "    # process and insert line into database\n",
    "    try:\n",
    "        # access the parameters of each line\n",
    "        if (len(line) == 6):\n",
    "            stationID = int(line[0]) # station identification number\n",
    "            timestamp = CreateDateTime(line[1]) # measurement time\n",
    "            qn_7 = int(line[2]) # quality level of next columns\n",
    "            ww = int(line[3])\n",
    "            #ww_text will be ignored\n",
    "            eor = line[5] # end of record\n",
    "\n",
    "            if (int(timestamp.year) >= offsetYear):\n",
    "                # create unix timestamp\n",
    "                unixTimestamp = int(timestamp.timestamp())\n",
    "\n",
    "                # save line in database\n",
    "                return (stationID, qn_7, timestamp, unixTimestamp, ww)\n",
    "            else:\n",
    "                # return nothing in case of an error\n",
    "                return None\n",
    "    except:\n",
    "        print('Error processing line:', line)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 4 - stundenwerte_WW_01975_19950905_20211231_hist.zip\n",
      "Process file 2 of 4 - stundenwerte_WW_02171_19950901_20211231_hist.zip\n",
      "Process file 3 of 4 - stundenwerte_WW_02261_19991202_20211231_hist.zip\n",
      "Process file 4 of 4 - stundenwerte_WW_02483_19951012_20211231_hist.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeHistoricalData):\n",
    "    # move to ftp directory and retrieve files\n",
    "    ftp.cwd(pathHistorical)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine, utf8=False)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 4 - stundenwerte_WW_01975_akt.zip\n",
      "Process file 2 of 4 - stundenwerte_WW_02171_akt.zip\n",
      "Process file 3 of 4 - stundenwerte_WW_02261_akt.zip\n",
      "Process file 4 of 4 - stundenwerte_WW_02483_akt.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeRecentData):\n",
    "    # move to ftp directory\n",
    "    ftp.cwd(pathRecent)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine, utf8=False)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station observations of wind speed and wind direction for Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General settings/parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathHistorical = '/climate_environment/CDC/observations_germany/climate/hourly/wind/historical'\n",
    "pathRecent = '/climate_environment/CDC/observations_germany/climate/hourly/wind/recent'\n",
    "dataFileFilter = 'produkt_ff_stunde*.txt'\n",
    "sqlCommand = 'INSERT OR IGNORE INTO Wind(StationIdent, QLIdent, DateMeasuredUTC, DateMeasuredUnixUTC, FF, DD) VALUES (?, ?, ?, ?, ?, ?)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessFileLine(line, offsetYear):\n",
    "    # process and insert line into database\n",
    "    try:\n",
    "        # access the parameters of each line\n",
    "        if (len(line) == 6):\n",
    "            stationID = int(line[0]) # station identification number\n",
    "            timestamp = CreateDateTime(line[1]) # measurement time\n",
    "            qn_3 = int(line[2]) # quality level of next columns\n",
    "            ff = float(line[3])\n",
    "            dd = int(line[4])\n",
    "            eor = line[5] # end of record\n",
    "\n",
    "            if (int(timestamp.year) >= offsetYear):\n",
    "                # create unix timestamp\n",
    "                unixTimestamp = int(timestamp.timestamp())\n",
    "\n",
    "                # save line in database\n",
    "                return (stationID, qn_3, timestamp, unixTimestamp, ff, dd)\n",
    "            else:\n",
    "                # return nothing in case of an error\n",
    "                return None\n",
    "    except:\n",
    "        print('Error processing line:', line)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 6 - stundenwerte_FF_01975_19500101_20211231_hist.zip\n",
      "Process file 2 of 6 - stundenwerte_FF_02171_19760401_20211231_hist.zip\n",
      "Process file 3 of 6 - stundenwerte_FF_02261_19480101_20211231_hist.zip\n",
      "Process file 4 of 6 - stundenwerte_FF_02315_20090101_20211231_hist.zip\n",
      "Process file 5 of 6 - stundenwerte_FF_02437_19710101_20080131_hist.zip\n",
      "Process file 6 of 6 - stundenwerte_FF_02483_19690101_20211231_hist.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeHistoricalData):\n",
    "    # move to ftp directory and retrieve files\n",
    "    ftp.cwd(pathHistorical)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file 1 of 5 - stundenwerte_FF_01975_akt.zip\n",
      "Process file 2 of 5 - stundenwerte_FF_02171_akt.zip\n",
      "Process file 3 of 5 - stundenwerte_FF_02261_akt.zip\n",
      "Process file 4 of 5 - stundenwerte_FF_02315_akt.zip\n",
      "Process file 5 of 5 - stundenwerte_FF_02483_akt.zip\n"
     ]
    }
   ],
   "source": [
    "if (IncludeRecentData):\n",
    "    # move to ftp directory\n",
    "    ftp.cwd(pathRecent)\n",
    "    files = ftp.nlst()\n",
    "\n",
    "    # filter files\n",
    "    filteredFiles = FilterFileList(stations, files)\n",
    "\n",
    "    # process ftp server files\n",
    "    ProcessFTPData(filteredFiles, dataFileFilter, recordStartYear, sqlCommand, workDirectory, db, ProcessFileLine)\n",
    "\n",
    "    # save changes in local sqlite database\n",
    "    db.commit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close ftp connection\n",
    "ftp.quit()\n",
    "\n",
    "# close db connection\n",
    "db.commit()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f15f7e9e254e7021a521710cdb6aab6ce7db0a7ba9ad7916aaeba5e8044a243"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
